#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Mar 30 11:43:59 2020

Regularized LDA
The code was implemented based on the book:
Element of statistical learning.
Trevor Hastie, Robert Tibshirani, Jerome Friedman
Pag 106 - 119

@author: raschell
"""

# Import numpy lib
import numpy as np
# Import cholesky lib
from scipy.linalg import cholesky
# Import sklearn base classes
from sklearn.base import BaseEstimator
# Import sklearn accuracy metrics lib
from sklearn.metrics import accuracy_score, mutual_info_score


def _mean_cov(X, y):
    """Compute class mean and covariance matrix.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Input data.

    y : array-like, shape (n_samples,) or (n_samples, n_targets)
        Target values.

    Returns
    -------
    cov : array-like, shape (n_features, n_features)
        Covariance matrix.
    mean : array-like, shape (n_features, )
        Class mean matrix.
    """
    
    classes = np.unique(y)
    cov_tmp = []
    mean_tmp = []
    for group in classes:
        mean_class = np.mean(X[y == group, :],0)
        mean_tmp.append(mean_class)
        cov_tmp.append(X[y == group, :] - np.tile(mean_class,(sum(y == group),1)))
    
    mean = np.array(mean_tmp)
    cov_tmp = np.concatenate(cov_tmp, axis = 0)
    cov = np.cov(cov_tmp.T)
    
    return cov, mean

def _class_means(X, y):
    """Compute class means.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Input data.

    y : array-like, shape (n_samples,) or (n_samples, n_targets)
        Target values.

    Returns
    -------
    means : array-like, shape (n_classes, n_features)
        Class means.
    """
    classes, y = np.unique(y, return_inverse=True)
    cnt = np.bincount(y)
    means = np.zeros(shape=(len(classes), X.shape[1]))
    np.add.at(means, y, X)
    means /= cnt[:, None]
    return means


class rLDA(BaseEstimator):
    '''
    Linear Discriminant Analysis

    A classifier with a linear decision boundary, generated by fitting class
    conditional densities to the data.

    The model fits a Gaussian density to each class, assuming that all classes
    share the same covariance matrix.

    The fitted model can also be used to reduce the dimensionality of the input
    by projecting it to the most discriminative directions.
    
    Parameters
    ----------
    regression_coeff : float, optional
        Regression coefficient. The default is 0.01.
        
    threshold_detect : float, optional
        Probability threshold for detectig an event. The default is 0.8.
        
    refractory_period: int/float, optional
        Minimum number of samples between 2 events of the same type. The default is 0.
    
    Examples
    --------
    >>> import numpy as np
    >>> from decoder import rLDA
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> clf = rLDS()
    >>> clf.fit(X, y)
    
    '''
    
    def __init__(self, regression_coeff = 0.01, threshold_detect = 0.8, refractory_period = 0):
        if (regression_coeff < 0) or (regression_coeff > 1):
            raise Exception('ERROR: regression_coeff < 0 or > 1!')
        if (threshold_detect < 0) or (threshold_detect > 1):
            raise Exception('ERROR: threshold_detect < 0 or > 1!')
            
        self.regression_coeff = regression_coeff
        self.threshold_detect = threshold_detect
        self.refractory_period = refractory_period

    def detect_class(self, probs):
        '''
        Predict output class based on probabilities.

        Parameters
        ----------
        probs : array-like, shape (n_samples, n_classes)
            Classes probabilities.
            
        Return
        ----------
        C : array, shape (n_samples, )
            Classes prediction.
        
        '''
        
        n_samples, n_classes = probs.shape
        
        C = -np.ones((n_samples,))
        for group in self.classes_:
            events = np.where((probs[:,group] > self.threshold_detect))[0]
            if events.size != 0:
                # Remove event repetition based on refractory period
                events_valid = np.ones(len(events)).astype('bool')
                for iEv, event in enumerate(events):
                    if events_valid[iEv] == True:
                        events_valid = np.logical_and(events_valid,np.logical_not(np.logical_and(events>event, events<event+self.refractory_period)))
                C[events[events_valid]] = group
        
        return C

    def fit(self, X, y):
        '''
        Train rLDA

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training data.

        y : array-like, shape (n_samples,)
            Target values.

        '''
        
        if X.ndim != 2:
            raise Exception('ERROR: X must be 2 dimensional (n_samples, n_features)!')
        
        self.classes_ = np.unique(y).astype('int')
        n_classes = len(self.classes_)
        n_samples, n_feature = X.shape

        if n_samples == n_classes:
            raise ValueError("The number of samples must be more "
                             "than the number of classes.")
        
        # Maximum number of components
        self._max_components = min(n_classes - 1, X.shape[1])

        # Get mean for each class and classes covariance matrix
        self.class_cov_, self.mean_ = _mean_cov(X,y)
        # Compute overall cov matrix
        self.cov_ = (1 - self.regression_coeff) * self.class_cov_ + (self.regression_coeff / n_feature) * np.trace(self.class_cov_) * np.eye(self.class_cov_.shape[0])
        # Compute cov inverse
        self.inv_cov_ = np.linalg.inv(self.cov_)
        try:
            self.chol_cov_ = cholesky(self.inv_cov_)
        except:
            self.chol_cov_ = cholesky(self.inv_cov_ + self.inv_cov_.T)

    def predict(self, X, probs = False):
        '''
        Predict output class.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Test data.

        proba : bool, optional
            Return the classes probability.

        Return
        ----------
        C : array, shape (n_samples, )
            Classes prediction.
            
        probabilities : array, shape (n_samples, n_classes)
            Estimated classes probabilities.
        
        '''
                
        n_samples, n_features = X.shape
        n_classes = len(self.classes_)
        if n_features != self.chol_cov_.shape[0]:
            raise Exception('ERROR: n_features in X {} != from features in chol_cov {}!'.format(n_features, self.chol_cov_.shape[0]))
        
        log_probabilities = []
        for group in self.classes_:
            distVect = X - np.tile(self.mean_[group,:], (n_samples, 1))
            log_probabilities.append(-np.sum(np.matmul(self.chol_cov_,distVect.T)**2,axis = 0)/2)
        
        log_probabilities = np.array(log_probabilities)
        probabilities = []
        for group in self.classes_:
            log_prob_class = log_probabilities - np.tile(log_probabilities[group,:], (n_classes,1))
            probabilities.append(1./np.sum(np.exp(log_prob_class),axis = 0))
        
        probabilities = np.array(probabilities).T
        
        if probs:
            return probabilities
        else:
            C = self.detect_class(probabilities)
            return C

    def predict_proba(self, X):
        '''
        Predict output class probabilities.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Test data.

        Return
        ----------
        probabilities : array, shape (n_samples, n_classes)
            Estimated classes probabilities.
        
        '''
        return self.predict(X, probs = True)

    def get_labels(self):
        '''
        This function returns the labels.

        Returns
        -------
        labels : array-like, shape (n_classes, )
            Labels of the classes how they are stored and decoded.

        '''
        return self.classes_

    def score(self, X, y_true, metric = 'accuracy'):
        '''
        Predict output class probabilities.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Test data.
            
        y_true : array-like, shape (n_samples,)
            Target values.

        Return
        ----------
        score : 
            Decoding accuracy.
        
        '''
        
        if metric not in ['accuracy','mutual_info']:
            raise ValueError('ERROR: metric must be either "accuracy" or "mutual_info"!')
        
        y_pred = self.predict(X, probs = False)
        
        if metric == 'accuracy':
            score = accuracy_score(y_true, y_pred, normalize = True)
        elif metric == 'mutual_info':
            score = mutual_info_score(y_true, y_pred)

        return score 
        
# EOF